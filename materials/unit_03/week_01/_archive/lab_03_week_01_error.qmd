---
title: "Week 13 Lab: Introduction to Panel Data Analysis and Spatial Data in R"
format: 
  html:
    theme: zephyr
    toc: true
---


![](includes/long_ai.jpeg){height=60%, fig-align="center}

::: {style="font-size: 1.5em"}
This Lab Contributes to Course Objectives: 1, 3, 4, 5, 7, 8
:::

<!-- In spring 2024, Lauren moved the R section on spatial data from lab week 11 to lab week 13 -->

## Learning Objectives R

- Read in and manipulate spatial data in R

- Join spatial data

- Prepare data for mapping in Tableau

- Prepare data for regression analysis

# Introduction to Spatial Data in R

Spatial data is a form of cross-sectional data that contains geographic information.^[Spatial data can also have temporal dimensions, and the concepts we cover here extend to those data too.] Examples include the location of a store or park, the boundary of a parcel of land, or the location of a weather measurement. In many cases, we need to process that data in some way or join it to other (potentially spatial) data for analysis.

## Spatial Data Formats

Two common types of spatial data in R are **vector** and **raster** data. 

- Vector data represent geographic features as points, lines, and polygons. Vector data is often used to represent discrete features such as the boundaries of a city or the location of a specific point of interest. It is common to find data with specific location information (e.g., latitude and longitude coordinates). 

- Raster data represent geographic features as a grid of cells with values assigned to each cell. Raster data, on the other hand, is often used to represent phenomena such as elevation or temperature that are continuous across the landscape.

## R Packages

Working with spatial data in R involves using specialized packages and functions to read, manipulate, and visualize the data. Some popular packages for working with spatial data in R include `sf` and `raster`. With these tools, it's possible to perform a wide range of spatial analyses, such as overlaying different layers of data to find areas of overlap or proximity, extracting data for specific regions of interest, and creating custom maps and visualizations. Taro Meino has written a useful reference on using [R for GIS](https://tmieno2.github.io/R-as-GIS-for-Economists/).

## Objective

We will develop a spatial dataset in which you join convenience store location data with county level data from the U.S. Census and weather data from NOAA. 

::: {.callout-important title="Unit of Analysis"}
Always be paying attention to what is your **unit of analysis** or **unit of observation**. In this lab, our unit of analysis is the **county**, so we need to associate each convenience store with a county.
:::

## Setup

Our first step is to start an R script and load packages. 

<!-- We start by reading in the convenience store data set we have been using, along with population data from [SEDAC](https://sedac.ciesin.columbia.edu/data/set/popdynamics-us-county-level-pop-projections-sex-race-age-ssp-2020-2100/data-download). 
pop_raw <- read_csv("https://csu-arec-330.github.io/materials/unit_02/inputs/hauer_county_totpop_SSPs.csv")
-->

```{r}
#| eval: false

# This is the R script for Week 13 Lab. 
# This script will demonstrate how to work with spatial data and conduct some basic operations (e.g., intersection)

setwd("set my working directory")
getwd() # Confirm I am in the proper working directory.

# load packages
library(pacman)
p_load(tidyverse,janitor,sf,tigris,mapview,tidycensus,dplyr)

# Read in convenience store location dataset
store_raw <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/store_info.csv.gz")
```

## Converting Data to a Spatial Format

The field of geography and **geographic information systems (GIS)** specialize in understanding and working with geospatial data. Locations are known because of a commonly understood reference system called a **Coordinate Reference System (CRS)**. Think about a graph with $x$ and $y$ dimensions. A point on that graph has a set of coordinates that tell you the location. CRS in GIS can be more complicated because we are thinking about points that exist on a globe (this would be easier if the earth were flat). 

The `store_raw` dataframe has latitude and longitude coordinates. At this point, R considers these numbers like any other numeric variable. There are packages built to understand and work with geospatial data. `sf` stands for simple features and is a commonly used package. We will use it to convert the `store_raw` data into a spatially aware data frame.

```{r}
#| eval: false

# Convert data frame to a points simple feature object
store_geo <- store_raw %>%
  
  # First, we subset the variables `store_id` and the coordinates.
  select(store_id, latitude, longitude) %>% 
  
  # Tells R that these are essentially $x$ and $y$ coordinates in a certain projection (represented by the code 4326). See <https://epsg.io/> for more information.
  st_as_sf(coords=c("longitude","latitude"), crs = st_crs(4326))  

```

**Result:** `store_geo` is a different object than we have worked with before but it looks and behaves like a familiar dataframe. You can mutate new variables, arrange, and even summarize, but summarize also performs geospatial operations because you are changing the unit of analysis.

It is often helpful to look at data especially when mapping. R can leverage very powerful mapping tools, but we will just use them to make sure we are doing what we think we are doing. We can use `mapview()` to look at the data. 

```{r}
#| eval: false
# Configure mapview to render points directly in the RStudio Viewer (not using flatgeobuf)
mapviewOptions(fgb = FALSE)

# Display the store locations as an interactive map
mapview(store_geo)
```

> After you plot these points, what do you observe about the locations of the stores? What might you want to do with your data to remove outliers? Consider these answers as we move into the next section.

## Getting Census Boundaries

The dataframe `store_raw` contains a lot of meta information but not county. 

Boundaries for commonly used geometries (such as county boundaries) are available from the US Census and accessible through an **API** to the Census repository of these boundaries (called Tiger). The R package `tigris` provides a very convenient API to access them. 

```{r}
#| eval: false

# Fetch data for county polygons using the 'tigris' package
us_co <- tigris::counties(cb = TRUE, class = "sf") %>%
  
  # Clean column names to snake_case using janitor
  janitor::clean_names() %>%
  
  # Convert land area from square meters to square miles
  mutate(aland = aland / 2.59e+6) %>%
  
  # Transform the coordinate reference system to WGS84 (EPSG:4326)
  st_transform(4326)

```

This command extracts the county boundaries for all states and reads them into an sf object called `us_co`.  

**What did this code do?**

1. The names are capitalized, so I clean them using `clean_names()` from the janitor package.
2. The land area reported is in square meters so I convert it to square miles.
3. I want to ensure that the coordinates are projected in the same coordinate reference system as our store locations. 

> View the dataframe `us_co`. Notice that the spatial type of these data are POLYGONs rather than POINTs.

::: {.callout-note}
If for some reason, there is an error, you can also load the county boundary from the course website: 

```{r}
#| eval: false
us_co <- st_read("https://csu-arec-330.github.io/materials/unit_03/inputs/us_co.gpkg")
```
:::


### You Do It

Use `mapview()` to plot the county layer.

```{r}
#| eval: false

# Use mapview to plot the county layer
mapview(us_co)

```

::: {.callout-note title="U.S. Geographies"}
Note that the sf object `us_co` contains information about the counties in the U.S., including county name, state, the area covered by land and water, and some numeric codes.

Since counties in different states can have the same name, a 5-digit county identifier known as a [Federal Information Processing Standard (FIPS)](https://www.smarty.com/articles/county-fips-codes) uniquely identifies counties. 

This code is labeled `geoid` in the `us_co` object. The first two digits of the fips are the state and the next three identify a county. Larimer county in CO has the FIPS code 08069. This fips code is used to join county-level data.
:::

## Removing Spatial Outliers

When analyzing spatial data, for tractability's sake, we might only want to look at data for the *contiguous* United States. So, it's often useful to remove territories and states that are not part of the "lower 48." States like Alaska and Hawaii, as well as territories such as American Samoa, Guam, and Puerto Rico, differ significantly in their geographical context and may introduce anomalies or skew the results when evaluating nationwide trends or metrics. Excluding them helps to maintain a focused analysis on the contiguous states where geographical and demographic characteristics are more uniform. This simplification can be particularly beneficial in analyses that involve spatial relationships or distance calculations, where the vast distances to these non-contiguous areas can distort results.

```{r}
#| eval: false

# Look at the distinct state list by statefp code
unique_statefp <- us_co %>%
  st_set_geometry(NULL) %>%  # Remove geometry column
  select(stusps, statefp) %>%  # Select the columns of interest
  distinct() %>%  # Remove duplicates, keeping only unique rows
  arrange(stusps)  # Arrange alphabetically by stusps

# Filter out counties from American Samoa (60), Guam (66), Saipan Municipality (69), Puerto Rico (72), Virgin Islands (78), Alaska (02), and Hawaii (15)
us_co_filtered <- us_co %>%
  filter(!statefp %in% c("60", "66", "72", "02", "15", "69", "78"))

# Use mapview to plot the county layer, excluding the specified states
mapview(us_co_filtered)

```

## Intersecting Points with Polygons

Spatial data processing tools can understand the shared location of points and polygons to associate data from one dataset with another.  

**Objective:** Associate a county name and identifier with each convenience store.

![](https://pygis.io/_images/overlay_intersects.jpg)

```{r}
#| eval: false

# Join county to store_geo
store_co_geo <- st_join(store_geo, us_co_filtered, join=st_intersects)

```

> This operation may take some time depending on the machine you are working on. Just be patient.

Notice that `store_co_geo` has all of the observations from `store_geo` and has the corresponding variables from `us_co` attached to it. Now we have county information associated with the store location. We can join the convenience store data to census data or other data at the county level via the 5-digit FIPS. We can also aggregate convenience store information up to the county level using `group_by() %>% summarize()`. 

### You Do It

Aggregate convenience store information up to the county level using `group_by() %>% summarize()` and plot the number of stores per county using mapview. 

```{r}
#| eval: false
#| code-fold: true

# Aggregate store count by county
store_count_by_county <- store_co_geo %>%
  group_by(geoid) %>%
  summarize(store_count = n(), .groups = 'drop')  # Drop groups to prevent regrouping

# Join aggregated data back with county geometries for mapping
county_store_map <- st_join(us_co_filtered,store_count_by_county,join=st_intersects)

# Visualize the result with mapview (showing number of stores per county)
mapview(county_store_map, zcol = "store_count")

```


## Accessing U.S. Census data

The US Census contains a wealth of data that can be used in analysis. We will access **median household income** from the American Community Survey, an annual survey conducted to supplement the decennial census, using the R package called `tidycensus` that conveniently wraps the Census API (<https://walker-data.com/tidycensus/>). The Census stores many datasets, so we need to identify the one we want to use to represent median household income. Tidycensus provides a utility to find the table we want:

```{r}
#| eval: false

# Load variable metadata for the 2022 ACS 5-year estimates (used to look up variable codes and labels)
census_22 <- load_variables(2022, "acs5", cache = TRUE)

```

Use the filter feature in the R Studio viewer or export the data as a csv that you can open in Excel and search the descriptions. As you can see, there are **many** other demographic tables available in the Census data. In this case, the median household income code is `B19013_001` (the table is B19013 and the variable we want is 001). We can access the data using the function `get_acs()`. 


```{r}
#| eval: false

# Download median household income data from the 2022 ACS 5-year estimates at the county level
census_hhi <- get_acs(
  geography = "county",                      # Get data for all U.S. counties
  survey = "acs5",                           # Use 5-year ACS data (more reliable for small areas)
  variables = c(medincome = "B19013_001"),   # B19013_001 = median household income
  state = NULL,                              # NULL = include all states (not just one)
  year = 2022                                # Use the most recent available year
)

# Clean column names and keep only GEOID and the income estimate, renamed as 'hhi'
hhi <- census_hhi %>%
  clean_names() %>%
  select(geoid, hhi = estimate)

```

This call to the API downloads county names and identifiers along with the variable estimate and margin of error. 

### Joining with Convenience Store County

We can join median household income to the stores by the common variable, `geoid`. 

```{r}
#| eval: false

# Join median household income (hhi) to the store-level dataset using county GEOID
store_hhi <- store_co_geo %>%
  st_set_geometry(NULL) %>%        # Remove geometry to work with as a regular data frame
  inner_join(hhi, by = "geoid")    # Join on county identifier (GEOID)

```

We have added median household income to our store-level data. Export this data and visualize in Tableau.

## Aggregating by Geography

Now we have the information to count the number of store in each county. We can use the `group_by()` and `summarize()` combination to calculate aggregate measures by county. Moreover, we don't need the spatial information anymore since everything will be at the county level. We use `st_set_geometry(NULL)` to convert an sf object back to a data frame (without spatial information), then add the number of stores in a county.

```{r}
#| eval: false

# Count the number of stores per county by joining spatial store data with county identifiers
county_stores <- store_co_geo %>%
  st_set_geometry(NULL) %>%            # Drop geometry to simplify data manipulation
  group_by(geoid, county = name, state = stusps) %>%  # Group by county GEOID, name, and state abbreviation
  summarize(total_stores = n()) %>%    # Count the number of stores in each county
  ungroup()                            # Remove grouping to avoid unexpected behavior in downstream steps

```

The result, `county_stores`, contains the number of convenience stores in each county.

## Creating a Panel

We may want to join our data to other data that varies over time. Suppose we are interested in how total sales at a store correlates with high temperatures. Weather data varies over time and space; however, store locations are fixed and do not change over time. We can use our existing data `stores_co_geo`, which has county information associated with each store, and join it to `shopper_info`, which contains the transaction level information. We need to aggregate the transaction level data up to sales per store-day.

```{r}
#| eval: false

# Read in the transaction level data
shopper_info <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/shopper_info.csv.gz")

# Read in the weather data
weather_raw <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/gridmet_county_daily_2023-2023.csv.gz")

```

`shopper_info` contains `store_id`, which we can use to join to `store_co_geo`, and `date_time`. 

First, extract the date component as we are not concerned with the time of day. Then, we group by `measure_date` and `store_id` and sum sales (`unit_price`*`unit_quantity`)

```{r}
#| eval: false

# Aggregate daily sales by store from transaction-level data
store_sales <- shopper_info %>%
  mutate(measure_date = as_date(date_time)) %>%              # Extract date component from timestamp
  group_by(store_id, measure_date) %>%                       # Group by store and date
  summarize(
    sales = sum(unit_price * unit_quantity, na.rm = TRUE)   # Compute total sales (price × quantity)
  ) %>%
  ungroup()                                                  # Remove grouping to clean up the result

```

`store_sales` contains the total sales by date for each store in the dataset. We can now join `store_sales` with county information associated with each store.

```{r}
#| eval: false

# Add county GEOID to the store-level sales data (so we can link to county-level data like weather)
store_sales_co <- st_set_geometry(store_co_geo, NULL) %>%
  select(store_id, geoid) %>%                      # Keep only store_id (for joining) and geoid (for county-level linkage)
  inner_join(store_sales, ., by = "store_id")      # Join to sales data using store_id; keep only stores with known county info

```
 
We have county-day measurements of weather. However, the data is in long format (as opposed to wide). The unit of observation of `weather_raw` is county-day-weather_measurement, but we need it to be county-day with different measurements as columns. We can reshape this data from long to wide using the function `pivot_wider()`. 

The next step is to join the weather data with `store_sales_co`. However, the columns we need to join on have different names. We can rename them or tell R how to map the names. 

::: {.callout-important title="Important Points before Joining Dataframes"}
1. The order of the variable name matters: The variable name on the left should be from the dataframe listed first. 
2. The weather data is for the continental US so store locations outside of that will be dropped because of the `inner_join()`.  
:::

```{r}
#| eval: false

# Reshape weather data from long to wide format (each variable becomes a column)
weather_wide <- weather_raw %>%
  pivot_wider(
    id_cols = c(county, date),              # Keep county and date as identifiers
    names_from = variable,                  # Each unique variable becomes a new column
    values_from = value                     # Fill those columns with the corresponding values
  )

# Join daily store sales to weather data using date and county identifiers
store_sales_weather <- store_sales_co %>%
  inner_join(weather_wide, by = c("measure_date" = "date", "geoid" = "county"))  # Match on date and county FIPS

```

## Summary

This lab demonstrates how to join spatial point data (convenience store locations) with spatial polygon data (county boundaries). Standard political boundaries like counties can be used as a common identifier to join US Census data. The lab also covered joining data that varies over space and time.

::: {.callout-note title="Lab Script for Week 13: Introduction to Panel Data Analysis and Spatial Data in R" collapse=true}

```{r}
#| eval: false

# This is the R script for Week 13 Lab. 
# This script will demonstrate how to work with spatial data and conduct some basic operations (e.g., intersection)

# load packages
library(pacman)
p_load(tidyverse,janitor,sf,tigris,mapview,tidycensus,dplyr)

# Read in convenience store location dataset
store_raw <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/store_info.csv.gz")

# Convert data frame to a points simple feature object
store_geo <- store_raw %>%
  
  # First, we subset the variables `store_id` and the coordinates.
  select(store_id, latitude, longitude) %>% 
  
  # Tells R that these are essentially $x$ and $y$ coordinates in a certain projection (represented by the code 4326). See <https://epsg.io/> for more information.
  st_as_sf(coords=c("longitude","latitude"), crs = st_crs(4326))  

# Configure mapview to render points directly in the RStudio Viewer (not using flatgeobuf)
mapviewOptions(fgb = FALSE)

# Display the store locations as an interactive map
mapview(store_geo)

# Fetch data for county polygons using the 'tigris' package
us_co <- tigris::counties(cb=T,class="sf") %>%
  janitor::clean_names() %>%
  mutate(aland=aland/2.59e+6) %>%
  st_transform(4326)

mapview(us_co)

# Look at the distinct state list by statefp code
unique_statefp <- us_co %>%
  st_set_geometry(NULL) %>%  # Remove geometry column
  select(stusps, statefp) %>%  # Select the columns of interest
  distinct() %>%  # Remove duplicates, keeping only unique rows
  arrange(stusps)  # Arrange alphabetically by stusps

# Filter out counties from American Samoa (60), Guam (66), Saipan Municipality (69), Puerto Rico (72), Virgin Islands (78), Alaska (02), and Hawaii (15)
us_co_filtered <- us_co %>%
  filter(!statefp %in% c("60", "66", "72", "02", "15", "69", "78"))

# Use mapview to plot the county layer, excluding the specified states
mapview(us_co_filtered)

# Join county to store_geo
store_co_geo <- st_join(store_geo, us_co_filtered, join=st_intersects)

# Aggregate store count by county
store_count_by_county <- store_co_geo %>%
  group_by(geoid) %>%
  summarize(store_count = n(), .groups = 'drop')  # Drop groups to prevent regrouping

# Join aggregated data back with county geometries for mapping
county_store_map <- st_join(us_co_filtered,store_count_by_county,join=st_intersects)

# Visualize the result with mapview (showing number of stores per county)
mapview(county_store_map, zcol = "store_count")

# Load variable metadata for the 2022 ACS 5-year estimates (used to look up variable codes and labels)
census_22 <- load_variables(2022, "acs5", cache = TRUE)

# Download median household income data from the 2022 ACS 5-year estimates at the county level
census_hhi <- get_acs(
  geography = "county",                      # Get data for all U.S. counties
  survey = "acs5",                           # Use 5-year ACS data (more reliable for small areas)
  variables = c(medincome = "B19013_001"),   # B19013_001 = median household income
  state = NULL,                              # NULL = include all states (not just one)
  year = 2022                                # Use the most recent available year
)

# Clean column names and keep only GEOID and the income estimate, renamed as 'hhi'
hhi <- census_hhi %>%
  clean_names() %>%
  select(geoid, hhi = estimate)

# Join median household income (hhi) to the store-level dataset using county GEOID
store_hhi <- store_co_geo %>%
  st_set_geometry(NULL) %>%        # Remove geometry to work with as a regular data frame
  inner_join(hhi, by = "geoid")    # Join on county identifier (GEOID)

# Count the number of stores per county by joining spatial store data with county identifiers
county_stores <- store_co_geo %>%
  st_set_geometry(NULL) %>%            # Drop geometry to simplify data manipulation
  group_by(geoid, county = name, state = stusps) %>%  # Group by county GEOID, name, and state abbreviation
  summarize(total_stores = n()) %>%    # Count the number of stores in each county
  ungroup()                            # Remove grouping to avoid unexpected behavior in downstream steps

# Read in the transaction level data
shopper_info <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/shopper_info.csv.gz")

# Read in the weather data
weather_raw <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/gridmet_county_daily_2023-2023.csv.gz")

# Aggregate daily sales by store from transaction-level data
store_sales <- shopper_info %>%
  mutate(measure_date = as_date(date_time)) %>%              # Extract date component from timestamp
  group_by(store_id, measure_date) %>%                       # Group by store and date
  summarize(
    sales = sum(unit_price * unit_quantity, na.rm = TRUE)   # Compute total sales (price × quantity)
  ) %>%
  ungroup()                                                  # Remove grouping to clean up the result
# Add county GEOID to the store-level sales data (so we can link to county-level data like weather)
store_sales_co <- st_set_geometry(store_co_geo, NULL) %>%
  select(store_id, geoid) %>%                      # Keep only store_id (for joining) and geoid (for county-level linkage)
  inner_join(store_sales, ., by = "store_id")      # Join to sales data using store_id; keep only stores with known county info

# Reshape weather data from long to wide format (each variable becomes a column)
weather_wide <- weather_raw %>%
  pivot_wider(
    id_cols = c(county, date),              # Keep county and date as identifiers
    names_from = variable,                  # Each unique variable becomes a new column
    values_from = value                     # Fill those columns with the corresponding values
  )

# Join daily store sales to weather data using date and county identifiers
store_sales_weather <- store_sales_co %>%
  inner_join(weather_wide, by = c("measure_date" = "date", "geoid" = "county"))  # Match on date and county FIPS

```

:::



<!-- ## Intersecting Points with Polygons -->

<!-- Spatial data processing tools can understand the shared location of points and polygons to associate data from one dataset with another.   -->

<!-- **Objective:** Associate a county name and identifier with each grocery store so that we can count the number of stores in each county. -->

<!-- ![](https://pygis.io/_images/overlay_intersects.jpg) -->


<!-- ```{r} -->
<!-- #| eval: false -->

<!-- # Spatially join store points to counties (point-in-polygon join) -->
<!-- store_co_geo <- st_join(store_geo, us_co, left = FALSE) # Note: This will take a while because we are spatially joining thousands of observations. -->

<!-- # Optional: You can also make this faster by selecting only one state. -->
<!-- # Filter counties to just Texas (FIPS state code "48") -->
<!-- tx_co <- us_co %>%  -->
<!--   filter(statefp == "48") -->

<!-- # Spatially join store points to Texas counties (fast point-in-polygon join) -->
<!-- store_tx_geo <- st_join(store_geo, tx_co, left = FALSE) -->
<!-- # This will return only the store points that fall within a Texas county, along with the associated county attributes. -->

<!-- ``` -->

<!-- This tells `sf` to: -->

<!-- - join each point in store_geo with the county polygon it falls inside -->
<!-- - drop stores that don't fall inside any polygon (`left = FALSE`) -->

<!-- Notice that `store_co_geo` has all of the observations from `store_geo` and has the corresponding variables from `us_co` attached to it.  -->

<!-- Now we have the information to count the number of store locations in each county: -->

<!-- 1. The `group_by()` and `summarize()` combination can also be used to conduct spatial operations like mashing polygons together.  -->

<!-- 2. We don't need the spatial information anymore since everything will be at the county level. We use `st_set_geometry(NULL)` to convert an sf object back to a data frame (without spatial information). -->

<!-- ```{r} -->
<!-- #| eval: false -->

<!-- # Convert spatial object to a non-spatial object and join with other datasets -->
<!-- store_co <- store_co_geo %>% -->
<!--   st_set_geometry(NULL) %>% #converts back to dataframe -->
<!--   group_by(geoid) %>%  -->
<!--   summarize(stores=n()) %>% -->
<!--   ungroup()  #forgets the grouping information otherwise it can affect future operations -->

<!-- ``` -->

<!-- ## Merging County Data -->

<!-- Now all datasets are aggregated to the county level, so we can merge or join them together. -->

<!-- Many counties in the U.S. share the same name even in different states. Federal Information Processing Standard (FIPS) codes are a more reliable key to join on. Before joining the data together, we need ot process the individual datasets to prepare them for easy merging. -->

<!-- ```{r} -->
<!-- #| eval: false -->

<!-- # Create a reference table with county name and land area (no geometry) for merging -->
<!-- co_name_land <- store_co_geo %>%  -->
<!--   st_set_geometry(NULL) %>% #converts back to dataframe -->
<!--   select(geoid, aland, name) -->

<!-- # Process population projection data from SEDAC (https://sedac.ciesin.columbia.edu) -->
<!-- # Keep only GEOID and 2020/2050 values for SSP2 -->
<!-- pop <- pop_raw %>% -->
<!--   janitor::clean_names() %>% -->
<!--   select(geoid = geoid10, pop2020 = ssp22020, pop2050 = ssp22050) -->

<!-- # Join spatial store dataset with land area and population data by GEOID -->
<!-- analysis_ds <- store_co %>% -->
<!--   inner_join(co_name_land, by = "geoid") %>%  # adds land area and county name -->
<!--   inner_join(pop, by = "geoid") %>%           # adds population projections -->
<!--   distinct()                                  # removes duplicates -->

<!-- ``` -->

<!-- This dataset gives you all of the counties where at least one store is located along with the land area, county name, and population projections. You can continue using this dataset to analyze in R or export for use in Tableau. -->

<!-- > Check your understanding: What is the **unit of analysis** or **unit of observation** for the `analysis_ds` dataframe? -->

<!-- {{< video https://youtu.be/LeZTDS3MDBY aspect-ratio="16x9" >}} -->





<!-- The rest of the code below was originally part of week 11 lab notes, but as of Spring 2024 we only covered regression in unit 03. -->

<!-- # Tableau -->

<!-- ## Learning Objectives Tableau -->

<!-- - Create visualizations that highlight outliers in regression results -->

<!-- - Incorporate other important variables in your visualizations of regression results -->

<!-- ### Visualizing Regression Results in Tableau -->

<!-- Last time we discussed how to plot your linear regression output from R. Today we will go into more detail on improving the utility of your regression estimates as well as ideas for plotting predicted versus observed values. -->

<!-- #### 1. Annotate figures to highlight points that are outliers or far from your regression line -->

<!-- Let's connect to the dataset we used in R to conduct a regression analysis (I named this `analysis_ds.csv`).  -->

<!-- 1. Create a scatterplot that shows the relationship between the number of stores and the population in 2020 (divided by 1000, **how?**) -->

<!-- 2. Create a new calculated field that captures your regression output using *reasonable* values for median household income, land area, and unemployment. (Hint: what are the summary statistics for these variables?) -->

<!-- 3. Add the line that results from this calculated field to your scatterplot. -->

<!-- 4. Now let's add some annotation to highlight Maricopa county to explain to the audience why this is such an outlier. Drag the field `Name` to the Label card. Deselect the `show mark labels` option. Right click the outlier point, select `Mark Label` -> `Always Show`. -->

<!-- 5. You can also add some text to help explain this outlier. Right click the point again and select `Annotate` -> `Point`. You might add some text like `This county contains Pheonix, which has the highest population and number of stores in Arizona.` (Or whatever you want to say.) -->

<!-- 6. You can also add some color to highlight this point. Right click on the point and select `Create Set`. Name this set `Outlier`. Once this set is created you will see a new field called `Outlier`. Drag this to the Color card. Change the colors as you desire, and hide the legend. -->

<!-- 7. Finally, we can incorporate another dimension in our analysis using the `Size` card. Choose one of the other variables you used in your regression analysis (I will use land area) and drag it to the Size card. You can then play with opacity, colors, and the relative sizes to make your visualization more effective. -->

<!-- #### 2. Create visualizations to show differences between future predictions of the number of stores and current number of stores -->

<!-- In R you used projections of population size in 2050 to produce estimates of the number of stores that we expect each county to support in 2050. Let's explore some different ways of visualizing these data. -->

<!-- 1. Connect to the dataset we created in R with results from our projections and current store numbers (I named this `gs_all.csv`). You can either join this with the dataset used for the regression analysis on the geoid variable (to stay in the same workbook) or open a new workbook. -->

<!-- 2. Create a new calculated field equal to the difference between the number of (predicted) stores in 2050 and the number of (observed) stores in 2020. Name this `Store Diff`. -->

<!-- 3. Create one more calculated field that indicates whether there is an increase or decrease in the number of stores. Call this field `Change in Stores` and use an `IF` `THEN` statement to create it.  -->

<!-- **Bullet Graphs** -->

<!-- 4. First, let's create a bullet graph. Select the fields `Name` `Stores 2020` and `Stores 2050` and go to Show Me -> Bullet Graph. By default the bullet graph shades regions associated with different percentiles of the variable on the x-axis (here 60% and 80% values of the Stores in 2020), and adds lines that represent the values of our second variable (here Stores in 2050). -->

<!-- 5. Let's make the bullet graph more useful for our purposes. Click on one of the markers showing the value of Stores in 2050. Click `Format` and in the left format pane that appears select `Fill below` and select a shade of gray. Now your figure shows the number of current stores (in blue) and the number of future stores (in gray).  -->

<!-- 6. Now let's add some useful color to demonstrate which counties have expected growth in store numbers and which have expected losses. Drag your calculated `Change in Stores` field to the Colors card. (You will need to assign it as a dimension and discrete) Let's change the colors so that red is associated with a decrease in the number of stores and green is associated with an increase. -->

<!-- 7. Finally, you may want the colored bars to indicate the projected store values rather than current. We can do this quickly by right clicking the x-axis and selecting `Swap reference line fields`. -->

<!-- **Dumbbell Charts** -->

<!-- 8. Next let's create a dumbbell chart (what you created in R). Open a new sheet in your Tableau workbook. Drag `Name` to the Rows shelf and `Measure Values` to the Columns shelf. This will add all of our measures to our viz. Let's delete everything except Stores 2020 and Stores 2050. Note that the level of aggregation doesn't really matter because there is only one observation per county. -->

<!-- 9. Let's change this fram bars to points my changing the graph type from automatic to circle. Then drag `Measure Names` to the Color card to change the colors of these points to be associated with the number of stores at the two different time points. -->

<!-- 10. Now we need to add lines connecting these points. Drag the `Measure Values` field to the Columns shelf (next to the current one). Create dual x-axes, synchronize them, and hide the upper x-axis header. Now we have two plots, but both are doing the same thing. Change one of your plots to a line (from circle). By default, Tableau connects all the points across counties, but we can change this behavior by dragging `Measure Names` to the Path card. -->

<!-- 11. Let's again add some color to indicate increases and decreases. Drag your calculated `Change in Stores` field to the Color card. (You will need to assign it as a dimension and discrete) -->

<!-- 12. We might want this color to apply to both the lines and points instead of just the lines. To do this go to the card for your circle plot. Drag `Measure Names` to the Detail card. Drag your calculated `Change in Stores` field to the Color card. (You will need to assign it as a dimension and discrete). Then let's change this from a circle plot to a Shape plot and drag `Measure Names` to the Shape card. Choose some shapes that scream to you "beginning" and "end". -->

<!-- 13. Finally, you might want to add the option to hide Maricopa county to make your visualization more readable. Drag the `Name` field to the Filters card and deselect `Maricopa` -->

<!-- **Maps** -->

<!-- 14. Now let's create a filled map to highlight these changes. Open a new sheet in your workbook, select `Geoid` and `Store Diff` (your calculated field) and go to Show Me -> Filled map.  -->

<!-- 15. Tableau's default coloring is not incredibly useful here, so let's edit these. Go to your Color card and Edit Colors. Select the `Red-Green Diverging` color palette and click ok. This is a bit better, but because of the large change in Maricopa county, it's a little hard to see the negative changes. Let's go back to the Color editing window and select `Use Full Color Range`. -->

<!-- 16. Finally, it might be more useful to see the percentage change, rather than numeric change in the number of stores. Create a new calculated field called `Percent Change` using the equation `([Stores 2050]-[Stores 2020])/[Stores 2020]`. Create a new filled map following the previous steps to show percentage, rather than level, changes in store counts. -->

<!-- ### Visualizing Clusters in Tableau -->

<!-- Last time we showed you how to visualize your cluster analysis from R in Tableau. This time, let's look at Tableau's clustering capabilities. It turns out that you can apply K-means clustering in Tableau! Unfortunately you cannot apply other clustering techniques, which is a limitation of doing this in Tableau instead of R. But, you can quickly and effectively perform fairly simple clustering analysis within Tableau. Read more about Tableau's clustering algorithm (and see a tutorial) [here](https://help.tableau.com/current/pro/desktop/en-us/clustering.htm). -->

<!-- 1. Let's open a new sheet in the workbook connected to your analysis dataset (`analysis_ds.csv`). Select `Geoid` and Show Me a Filled Map. Drag all the fields used in your cluster analysis to the Detail card (`Aland`, `Median hh inc`, `Ssp2020`, `Stores`, and `Unemployment`). -->

<!-- 2. Go to the `Analytics` tab on the left and drag `Cluster` onto your map (drop it on the pop-up window that appears). You can let Tableau automatically determine the number of custers, or you can assign a number of clusters. (The link above offers details on how Tableau determines the optimal number of clusters.) -->

<!-- 3. By default, Tableau has created 4 clusters, but it is not immediately clear what these clusters represent. One fast way to understand these different clusters is by clicking the dropdown menu for our cluster variable and selecting `Describe Clusters`. Here you can see summary statistics across all the variables used to create the clusters. -->

<!-- 4. We can also investigate the meaning of our clsuters using scatterplots across our relevant variables. First, let's add our clusters to our fields by dragging it to the data pane. Next, let's duplicate our first scatterplot showing our regression results. Let's filter out Maricopa county to make this easier to see. And let's drag our new `Clusters` field to the Color card. Here, we are intersted in understanding if any clusters highlight counties that have few stores relative to their population size. Do any colors stand out to you? -->

<!-- 5. We can replicate this analysis for each of our variables, but we will have to recreate our regression line so that it changes over the appropriate variable. Duplicate your calculated field `Regression Line 1` and edit this new field. Let's name it `Regression Line 2` and use the equation `74.51933 + 0.46365*(100) -1.06666*([Median Hh Inc]/1000) -0.84796*(7577/1000) + 1.69241*(6)` to make this line change over median household income, rather than population.  -->

<!-- 6. Create a scatterplot that shows the relationship between the number of stores and the Median HH income and add this new regression line to your scatterplot. Again, we are interested in identifying counties that have few stores relative to their median household income. Do any clusters stand out to you? -->

<!-- 7. If your cluster analysis does not reveal anything useful, you will likely want to adjust the variables included in your cluster analysis or perhaps the number of clusters. -->
