---
title: "Week 14: <br>Regression Analysis and Hypothesis Testing <br>"
format: 
  revealjs:
    theme: [night, ../../custom.scss]
    slide-number: c/t
    width: 1400
    # logo: path.png
    # center: TRUE
execute:
  echo: true

---

## Agenda 

Linear Regression

Hypothesis testing

<!-- ============================= -->
<!-- (4-22-2024) Lauren moved material in from Week 06 (Lecture 01 Week 02) For Spring 2025, plan is to re-do Week 06 lecture to be more about forecasting and decomposition. I left the content in Week 05 commented out. -->
<!-- ============================= -->

## Recap {.scrollable}

Last week (week 13) we covered:

- Reading in spatial data
- Understanding and implementing spatial joins
- Building panel datasets for regression analysis

Why this is important: Researchers might want to control for geographic variation in the outcome variable.

This week: Apply regression analysis to panel data.

# Linear Regression

# 
::: {.r-fit-text }
**The objective:** <br>
We want to understand the <br> 
relationship between two variables.
:::

#
::: {.r-fit-text }
**The simple solution:** <br> 
Assume the relationship is linear <br> 
and estimate the "average" trend.
:::


## Linear regression: The intuition {.scrollable}

Imagine you're trying to figure out a relationship between two things:

- What is the relationship (correlation, association) between $x$ (explanatory variable) and $y$ (outcome)?

For instance, what is the association between the number of hours you study ($x$) and your test score ($y$)? 

- Hypothesis: The more you study, the better your score $\rightarrow$ positively related. 

Linear regression helps us **quantify** this relationship.

- For each additional hour studied, *by how many points* will my test score increase?

::: {.notes}

:::




## Equation of a line {.scrollable}

:::: {.columns}

::: {.column width="50%"}

Quick review: The equation of a line plotting the relationship between $y$ and $x$ is,
$$y=mx+b $$

Which variable represents the slope?

Which variable represents the intercept?

:::

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-width: 6
#| include: false

library(ggplot2,quietly = T)

df <- data.frame(x=seq.int(0,10,1),
           y=seq(2,22,length.out=11))
```

```{r}
#| echo: true
#| fig-width: 6
#| code-fold: true

ggplot(df, aes(x, y)) + 
  geom_line() +
  scale_y_continuous(
    breaks = seq.int(0, 24, 2),
    limits = c(0, 24),
    expand = c(0, 0)
  ) +
  scale_x_continuous(
    breaks = seq.int(0, 10, 2),
    expand = c(0, 0)
  ) +
  theme_bw(base_size = 15) +
  theme(
    axis.line = element_line(color = "black"),   # Draw axis lines
    panel.border = element_blank()               # Remove default plot border
  )
```
:::

::::


## Studying and Test Scores

```{r}
#| echo: false

library(pacman)

# Load ggplot2 package
p_load(tidyverse,modelsummary)

len=150
# Set seed for reproducibility
set.seed(20)

# Generate data
hours_studied <- rnorm(len,mean = 25,sd=18) #1:len  # Students studying between 1 to 50 hours
hours_studied <- hours_studied[between(hours_studied,0,100)]
#hours_studied <- ifelse(hours_studied>50,50,hours_studied)
test_scores <- 40 + .8 * hours_studied + rnorm(length(hours_studied), mean = 0, sd = 15)  # Basic linear model with noise

# Combine into a data frame
data <- data.frame(hours_studied, test_scores) %>%
  filter(between(test_scores,0,100))
```


:::: {.columns}

::: {.column width="50%"}

**Our data:** We observe hours studied and test scores

- Plotted on the right using `ggpairs()` (Always plot your data!)


:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6

p_load(GGally)

# Basic scatter plot with ggplot
ggpairs(data) 
  
```

:::

::::

## Scatter plot


:::: {.columns}

::: {.column width="50%"}

**Our data:** We observe hours studied and test scores

What is the apparent relationship?

- Slope? 
- Intercept?

$$y=mx+b $$

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6

# Basic scatter plot with ggplot
p1 <- ggplot(data, aes(x = hours_studied, y = test_scores)) +
  geom_point(alpha=.6) +  # Add points for scatter plot
  #geom_smooth(method = "lm", se = FALSE, col = "blue") +  # Add linear regression line
  labs(subtitle = "Relationship between Hours Studied and Test Scores",
       x = "Hours Studied",
       y = "Test Scores") +
  theme_bw(base_size = 15)  # Use a minimal theme for aesthetics

p1
```

:::

::::

## General Expression of a Regression Equation

The equation of a regression line is very similar to the equation of a line:

$$y= \beta_0 + \beta_1 x + \varepsilon ;~~ \varepsilon \sim N(0,1)$$

- $\beta_0$ is our intercept
- $\beta_1$ is our slope

#

::: {.r-fit-text}
How does OLS estimate regression coefficients? [Shiny app](https://tomicapretto.shinyapps.io/LeastSquaresRegression/)<br>

<iframe src="https://tomicapretto.shinyapps.io/LeastSquaresRegression/" width="100%" height="500px"></iframe>

:::
 
## What is Ordinary Least Squares (OLS)? {.scrollable}

- **OLS** is a method for estimating the relationship between one or more independent variables and a dependent variable.

- It finds the **best-fitting line** by minimizing the **sum of squared differences** between the observed values and the predicted values.

- The goal is to find coefficients (slopes and intercept) that make the model's predictions as close as possible to the actual data.

- Assumes that errors are normally distributed with constant variance and that there is a linear relationship between predictors and outcome.


## Estimate Regression {.scrollable}


:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: true
#| include: false

model1 <- lm(test_scores ~ hours_studied,data)

m_out <- modelsummary(model1,stars = TRUE,coef_rename = c("(Intercept)"="Intercept","hours_studied"="Hours Studied"),gof_map = c("nobs", "r.squared"))
```

```{r}
m_out
```

In lab, we will learn about `lm()` which is the function that estimates a linear regression that generates this regression output.
:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6

# Basic scatter plot with ggplot
p1 +  # Add points for scatter plot
  geom_smooth(method = "lm", se = FALSE, col = "blue") +  # Add linear regression line
  geom_vline(xintercept = 0, color = "black") +  # <- bold black y-axis at x = 0
  theme_bw(base_size = 15)
```

:::

::::


## True model vs estimates

We use the regression output to construct our linear regression model:

$$scores=40 + 0.8 * hours + \varepsilon ;~~ \varepsilon \sim N(0,1)$$
```{r}
#| echo: false


m_out
```


## How well does our model fit? {.scrollable}

R^2^ is a statistic used to measures the proportion of variance in the dependent variable that is predictable from the independent variables

$$ R^2 = \frac{Explained ~~ Variation}{Total ~~ Variation}  $$

The value of R^2^ ranges from 0 to 1:

- 0 means that the model explains none of the variability of the response data around its mean.

- 1 means that the model explains all the variability of the response data around its mean.

::: {.notes}

Intuitive Explanation
Imagine you are a teacher trying to understand the performance of your students on a test. You suspect that several factors like hours of study, attendance, and participation in class could influence test scores.

Here’s how R2  fits into this scenario:

Total Variation: First, look at the variation in test scores among all students. Some score high, some low, and many in the middle. This spread of scores is the "total variation" in your dataset.

Explained Variation: Now, suppose you create a statistical model that attempts to predict these scores based on the factors you think might affect them (study hours, attendance, etc.). After applying your model, some of this total variation in test scores will be "explained" by the factors in your model. For example, perhaps students who study more and attend regularly score higher.

Unexplained Variation: There's almost always some part of the variation that your model can't explain. Maybe some students do well on tests because they're just good test-takers or they have a strong background in the subject that wasn’t measured.

:::

## Putting the coefficients on trial {.scrollable}

Did we observe our estimate **by chance** *or* **can we trust it**?

**The Hypotheses:**

$H_0$ (Null Hypothesis): posits that the regression coefficient is equal to zero.

- There is no effect of the independent variable (e.g., hours studied) on the dependent variable (e.g., test scores). 

$H_A$ (Alternative Hypothesis): posits that the coefficient is not zero. 

- There there is an effect, and this effect is different from zero. The amount of studying does influence test scores.

## Testing the hypothesis

We use a t-test to determine whether the regression coefficients are significantly different from zero.

The t-statistic is a ratio. The numerator is the difference between the estimated coefficient and zero (or other null hypothesis). The denominator is the standard error of the coefficient. 


$$ t =  \frac{\beta - 0}{se} =  \frac{0.783 - 0}{0.09} = 8.7 $$

## Student's t-distribution 

The shaded area is the probability that we observe the value by chance (p-value)

```{r}
#| echo: false

# Set degrees of freedom
df <- nrow(data)-2

# Create a sequence of t-values
t_values <- seq(-5, 5, length.out = 300)

# Compute the density of these t-values
t_density <- dt(t_values, df)

# Create a data frame for plotting
plot_data <- data.frame(t_values, t_density)


# Plotting the t-distribution
ggplot(plot_data, aes(x = t_values, y = t_density)) +
  geom_line() +  # Draw the density line
  geom_area(data = plot_data %>% filter(t_values > 1.96), fill = "blue", alpha = 0.4) +  
  geom_area(data = plot_data %>% filter(t_values < -1.96), fill = "blue", alpha = 0.4) +  
  geom_vline(xintercept = 0,linetype="dashed") +
  labs(title = paste0("Density of t-distribution (df = ",df,")"),
       x = "t value",
       y = "Density") +
  theme_minimal(base_size = 15)  # Use a minimal theme

```


## Student's t-distribution 

Locating our t-statistic on the density of the t-distribution quantifies the probability that we observe our result by chance (p-val = 1.199041e-14)

```{r}
#| echo: false

# Set degrees of freedom
df <- nrow(data)-2

# Create a sequence of t-values
t_values <- seq(-5, 10, length.out = 300)

# Compute the density of these t-values
t_density <- dt(t_values, df)

# Create a data frame for plotting
plot_data <- data.frame(t_values, t_density)


# Plotting the t-distribution
ggplot(plot_data, aes(x = t_values, y = t_density)) +
  geom_line() +  # Draw the density line
  geom_point(aes(x=8.7,y=0)) +
  geom_area(data = plot_data %>% filter(t_values > 1.96), fill = "blue", alpha = 0.4) +  
  geom_area(data = plot_data %>% filter(t_values < -1.96), fill = "blue", alpha = 0.4) +  
  geom_vline(xintercept = 0,linetype="dashed") +
  labs(title = paste0("Density of t-distribution (df = ",df,")"),
       x = "t value",
       y = "Density") +
  theme_minimal(base_size = 15)  # Use a minimal theme


```

## 95% Confidence Interval

Reverse engineer the p-value of 0.05 to find the test statistic (1.96)


$$ 1.96 =  \frac{\beta - 0}{se}  $$

Then rearrange the t-statistics calculation

$$ upper = \beta + 1.96*se ; ~~~ lower = \beta - 1.96*se$$

## Inference - back to the trial

The p-val = 0.00000000000001199041 suggests the probability of observing the relationship between studying and test score by chance is small.

Formally, we reject the null hypothesis H0 (no relationship) at an alpha=.05

Our test does not prove that H1 is correct, but it provides strong evidence.

## Interpreting regression coefficients

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false


m_out
```

:::

::: {.column width="50%"}

A student that does not study is expected to get a score of 40.389.

A student's score should improve by 0.78 points per hour of additional studying.

How many hours should a student study to get 80 points or better?

:::

::::

## Predicting new values

We can use the coefficient estimates to calculate expected scores from new students based on study time

$$scores=40 + 0.78 * hours $$

## Multivariate regression

You can add additional independent variables (a.k.a. regressors).

$$ y=\alpha + \beta_1 X_1 + \beta_2 X_2 + ... +\varepsilon $$

The $\beta$'s are **conditional** on the other covariates.

Inference is similar.


## Summary

Regression is a tool to estimate relationships between variables (measurements)

When assumptions are met, the regression provides the best estimates of the relationship

Hypothesis testing helps us understand the quality of our estimates



# Project 3

```{r}
#| echo: true
#| include: false
#| code-fold: true

# load packages
library(pacman)
p_load(tidyverse,janitor,sf,tigris,mapview,tidycensus,dplyr)

# ==== Read in Spatial Data ==== #

# Read in convenience store location dataset
store_raw <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/store_info.csv.gz")

# Convert data frame to a points simple feature object
store_geo <- store_raw %>%
  
  # First, we subset the variables `store_id` and the coordinates.
  select(store_id, latitude, longitude) %>% 
  
  # Tells R that these are essentially $x$ and $y$ coordinates in a certain projection (represented by the code 4326). See <https://epsg.io/> for more information.
  st_as_sf(coords=c("longitude","latitude"), crs = st_crs(4326))  

# Fetch data for county polygons using the 'tigris' package
us_co <- tigris::counties(cb=T,class="sf") %>%
  janitor::clean_names() %>%
  mutate(aland=aland/2.59e+6) %>%
  st_transform(4326)

# Look at the distinct state list by statefp code
unique_statefp <- us_co %>%
  st_set_geometry(NULL) %>%  # Remove geometry column
  select(stusps, statefp) %>%  # Select the columns of interest
  distinct() %>%  # Remove duplicates, keeping only unique rows
  arrange(stusps)  # Arrange alphabetically by stusps

# Filter out counties from American Samoa (60), Guam (66), Saipan Municipality (69), Puerto Rico (72), Virgin Islands (78), Alaska (02), and Hawaii (15)
us_co_filtered <- us_co %>%
  filter(!statefp %in% c("60", "66", "72", "02", "15", "69", "78"))

# Join county to store_geo
store_co_geo <- st_join(store_geo, us_co_filtered, join=st_intersects)

# Aggregate store count by county
store_count_by_county <- store_co_geo %>%
  group_by(geoid) %>%
  summarize(
    store_count = n(), # Count the number of stores in each county
    .groups = 'drop')  # Drop groups to prevent regrouping

# Join aggregated data back with county geometries for mapping
county_store_map <- st_join(us_co_filtered, store_count_by_county, join=st_intersects)

# ==== Spatial Joins - Census ==== #

# Load variable metadata for the 2022 ACS 5-year estimates (used to look up variable codes and labels)
census_22 <- load_variables(2022, "acs5", cache = TRUE)

# Download median household income data from the 2022 ACS 5-year estimates at the county level
census_hhi <- get_acs(
  geography = "county",                      # Get data for all U.S. counties
  survey = "acs5",                           # Use 5-year ACS data (more reliable for small areas)
  variables = c(medincome = "B19013_001"),   # B19013_001 = median household income
  state = NULL,                              # NULL = include all states (not just one)
  year = 2022                                # Use the most recent available year
)

# Clean column names and keep only GEOID and the income estimate, renamed as 'hhi'
hhi <- census_hhi %>%
  clean_names() %>%
  select(geoid, hhi = estimate)

# Join median household income (hhi) to the store-level dataset using county GEOID
store_hhi <- store_co_geo %>%
  st_set_geometry(NULL) %>%        # Remove geometry to work with as a regular data frame
  inner_join(hhi, by = "geoid")    # Join on county identifier (GEOID)

# Join median household income (hhi) to the county-store map using county GEOID
county_hhi <- county_store_map %>%
  select(-geoid.y) %>%
  rename(geoid = geoid.x) %>%
  inner_join(hhi, by = "geoid")    # Join on county identifier (GEOID)


# ==== Spatial Joins - Weather ==== #

# Read in the transaction level data
shopper_info <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/shopper_info.csv.gz")

# Read in the weather data
weather_raw <- read_csv("https://csu-arec-330.github.io/materials/unit_03/inputs/gridmet_county_daily_2023-2023.csv.gz")

# ==== Store-related questions ==== #

# Aggregate daily sales by store from transaction-level data
store_sales <- shopper_info %>%
  mutate(gtin = ifelse(is.na(gtin), 0, gtin)) %>%
  mutate(measure_date = as_date(date_time)) %>%              # Extract date component from timestamp
  group_by(store_id, measure_date) %>%                       # Group by store and date
  summarize(
    sales = sum(unit_price * unit_quantity, na.rm = TRUE)   # Compute total sales (price × quantity)
  ) %>%
  ungroup()                                                  # Remove grouping to clean up the result

# Add county GEOID to the store-level sales data (so we can link to county-level data like weather)
store_sales_co <- st_set_geometry(store_co_geo, NULL) %>%
  select(store_id, geoid) %>%                      # Keep only store_id (for joining) and geoid (for county-level linkage)
  inner_join(store_sales, ., by = "store_id")      # Join to sales data using store_id; keep only stores with known county info

# Reshape weather data from long to wide format (each variable becomes a column)
weather_wide <- weather_raw %>%
  pivot_wider(
    id_cols = c(county, date),              # Keep county and date as identifiers
    names_from = variable,                  # Each unique variable becomes a new column
    values_from = value                     # Fill those columns with the corresponding values
  )

# Join daily store sales to weather data using date and county identifiers
store_sales_weather <- store_sales_co %>%
  inner_join(weather_wide, by = c("measure_date" = "date", "geoid" = "county"))  # Match on date and county FIPS

# Results in a data frame of daily sales at the store level with daily precipitation levels, humidity levels, and air temperature.
```


# 
::: {.r-fit-text }
**Option 1:** <br>
What is the relationship between <br> 
median income and total store count <br> 
at the county level?
:::

## Plot Your Data
```{r}
#| fig-width: 6
#| code-fold: true

p_load(GGally)

# Basic scatter plot with ggplot
county_hhi %>%
  st_set_geometry(NULL) %>%
  mutate(store_count = replace_na(store_count, 0)) %>%
  select(hhi, store_count) %>%
  ggpairs()
```

## Zoom in on Scatter Plot

```{r}
#| echo: false
#| fig-width: 6

county_hhi_slim <- county_hhi %>%
  st_set_geometry(NULL) %>%
  mutate(store_count = replace_na(store_count, 0)) %>%
  select(hhi, store_count)

# Basic scatter plot with ggplot
p1 <- ggplot(county_hhi_slim, aes(x = hhi, y = store_count)) +
  geom_point(alpha=.6) +  # Add points for scatter plot
  labs(subtitle = "Relationship between Median Household Income and Number of Stores",
       x = "Median Household Income (2022)",
       y = "Store Count (County Level)") +
  theme_bw(base_size = 15)  # Use a minimal theme for aesthetics

p1
```

## Estimate Regression

```{r}
#| echo: true
#| include: false
#| code-fold: true

library(modelsummary)

model1 <- lm(store_count ~ hhi, county_hhi_slim)

m_out <- modelsummary(model1,
                      fmt = 5,
                      stars = TRUE,
                      coef_rename = c("(Intercept)"="Intercept",
                                      "hhi"="Median Inc"),
                      gof_map = c("nobs", "r.squared"))
```

```{r}
m_out
```
:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6
#| code-fold: true

# Basic scatter plot with ggplot
p1 +  # Add points for scatter plot
  geom_smooth(method = "lm", se = FALSE, col = "blue") +  # Add linear regression line
  geom_vline(xintercept = 0, color = "black") +  # <- bold black y-axis at x = 0
  theme_bw(base_size = 15)
```
:::

## Linear Regression Model {.scrollable}

$$StoreCount= -4.38605 + 0.00019 * MedianIncome + \varepsilon ;~~ \varepsilon \sim N(0,1)$$
```{r}
#| echo: false

m_out
```

## Interpreting the Results {.scrollable}

1. The $R^2$ is very low at 0.046, meaning that median income does not explain much of the variation in store count. In general, this is not a well-fitted model.

2. The **intercept** ($\beta_0$) is −4.38605, meaning that when median income=0, the model predicts a negative store count. Since an income of 0 isn't meaningful, the intercept mainly acts as a mathematical anchor. (We usually don't over-interpret it here.)

3. The **slope** ($\beta_1$) is 0.00019, meaning that for every 1 unit increase in median income, store count increases by 0.00019.

- One way to make this more interpretable is to imagine that median income increases by \$10,000. Then, we would expect store count to increase by 1.9 (because 10,000 $\times$ 0.00019 = 1.9), which is almost 2 stores. At the county level, this could make sense.

## Action Items {.scrollable}

If we take these results to be true, we might suggest to a policymaker that **higher median income areas are slightly more likely to attract more stores**. 

However, given the low $R^2$, other factors likely play a much larger role in determining store count. 

Further steps could include:

- Using a multivariate model to better capture the drivers of store count variation by adding more predictors (e.g., population density, rural or urban status, infrastructure).
