---
title: "Week 14: <br>Regression Analysis and Hypothesis Testing <br>"
format: 
  revealjs:
    theme: [night, ../../custom.scss]
    slide-number: c/t
    width: 1400
    # logo: path.png
    # center: TRUE
execute:
  echo: true

---

## Agenda 

Linear Regression

Hypothesis testing

<!-- ============================= -->
<!-- (4-22-2024) Lauren moved material in from Week 06 (Lecture 01 Week 02) For next year, plan is to re-do Week 06 lecture to be more about forecasting and decomposition. I left the content in there for Week 05 (commented out) for now. -->
<!-- ============================= -->


# Linear Regression

# 
::: {.r-fit-text }
**The objective:** <br>
We want to understand the <br> 
relationship between two variables
:::

#
::: {.r-fit-text }
**The simple solution:** <br> 
Assume the relationship is linear <br> 
and estimate the "average" trend
:::


## Linear regression: The intuition {.scrollable}

Imagine you're trying to figure out a relationship between two things. 

For instance, how the number of hours you study affects your score on a test. 

Intuitively, you might think that the more you study, the better your score. 

Linear regression helps us quantify this relationship.

::: {.notes}

:::




## Equation of a line {.scrollable}

:::: {.columns}

::: {.column width="50%"}

Quick review: The equation of a line plotting the relationship between $y$ and $x$ is,
$$y=mx+b $$

Which variable represents the slope?

Which variable represents the intercept?

:::

::: {.column width="50%"}

```{r}
#| fig-width: 6

library(ggplot2,quietly = T)
data.frame(x=seq.int(0,10,1),
           y=seq(2,22,length.out=11)) |>
  ggplot(aes(x,y)) + 
    geom_line() +
    scale_y_continuous(breaks = seq.int(0,24,2),limits = c(0,24)) +
    scale_x_continuous(breaks = seq.int(0,10,2)) +
    theme_bw(base_size = 15)
```
:::

::::


## Studying and Test Scores

```{r}
#| echo: false


library(pacman)

# Load ggplot2 package
p_load(tidyverse,modelsummary)

len=150
# Set seed for reproducibility
set.seed(20)

# Generate data
hours_studied <- rnorm(len,mean = 25,sd=18) #1:len  # Students studying between 1 to 50 hours
hours_studied <- hours_studied[between(hours_studied,0,100)]
#hours_studied <- ifelse(hours_studied>50,50,hours_studied)
test_scores <- 40 + .8 * hours_studied + rnorm(length(hours_studied), mean = 0, sd = 15)  # Basic linear model with noise

# Combine into a data frame
data <- data.frame(hours_studied, test_scores) %>%
  filter(between(test_scores,0,100))
```


:::: {.columns}

::: {.column width="50%"}

We observe hours studied and test scores

Always plot your data


:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6

p_load(GGally)

# Basic scatter plot with ggplot
ggpairs(data) 
  
```

:::

::::

## Scatter plot


:::: {.columns}

::: {.column width="50%"}

We observe hours studied and test scores

Always plot your data

What is the apparent relationship?

Slope? Intercept?

$$y=mx+b $$

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6

# Basic scatter plot with ggplot
p1 <- ggplot(data, aes(x = hours_studied, y = test_scores)) +
  geom_point(alpha=.6) +  # Add points for scatter plot
  #geom_smooth(method = "lm", se = FALSE, col = "blue") +  # Add linear regression line
  labs(subtitle = "Relationship between Hours Studied and Test Scores",
       x = "Hours Studied",
       y = "Test Scores") +
  theme_bw(base_size = 15)  # Use a minimal theme for aesthetics

p1
```

:::

::::

#

::: {.r-fit-text}
How does OLS estimate regression coefficients? <br>
Shiny App
:::


## Estimate Regression {.scrollable}


:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: true

model1 <- lm(test_scores ~ hours_studied,data)

m_out <- modelsummary(model1,stars = TRUE,coef_rename = c("(Intercept)"="Intercept","hours_studied"="Hours Studied"),gof_map = c("nobs", "r.squared"))

m_out
```

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-width: 6

# Basic scatter plot with ggplot
p1 +  # Add points for scatter plot
  geom_smooth(method = "lm", se = FALSE, col = "blue")   # Add linear regression line
  
```

:::

::::


## True model vs estimates

$$scores=40 + 0.8 * hours + \varepsilon ;~~ \varepsilon \sim N(0,15)$$
```{r}
#| echo: false


m_out
```

## How well does our model fit? {.scrollable}

R^2^ is a statistic used to measures the proportion of variance in the dependent variable that is predictable from the independent variables

$$ R^2 = \frac{Explained ~~ Variation}{Total ~~ Variation}  $$

The value of R^2^ ranges from 0 to 1:

- 0 means that the model explains none of the variability of the response data around its mean.

- 1 means that the model explains all the variability of the response data around its mean.

::: {.notes}

Intuitive Explanation
Imagine you are a teacher trying to understand the performance of your students on a test. You suspect that several factors like hours of study, attendance, and participation in class could influence test scores.

Here’s how R2  fits into this scenario:

Total Variation: First, look at the variation in test scores among all students. Some score high, some low, and many in the middle. This spread of scores is the "total variation" in your dataset.

Explained Variation: Now, suppose you create a statistical model that attempts to predict these scores based on the factors you think might affect them (study hours, attendance, etc.). After applying your model, some of this total variation in test scores will be "explained" by the factors in your model. For example, perhaps students who study more and attend regularly score higher.

Unexplained Variation: There's almost always some part of the variation that your model can't explain. Maybe some students do well on tests because they're just good test-takers or they have a strong background in the subject that wasn’t measured.

:::

## Putting the coefficients on trial

Did we observe our estimate by chance or can we trust it?

**The Hypotheses:**

- Null Hypothesis (H0): posits that the regression coefficient is equal to zero - no effect of the independent variable (e.g., hours studied) on the dependent variable (e.g., test scores). 

- Alternative Hypothesis (H1): posits that the coefficient is not zero - there is an effect, and the amount of studying does influence test scores.


## Testing the hypothesis

We use a t-test to determine whether the regression coefficients are significantly different from zero.

The t-statistic is a ratio. The numerator is the difference between the estimated coefficient and zero (or other null hypothesis). The denominator is the standard error of the coefficient. 


$$ t =  \frac{\beta - 0}{se} =  \frac{0.783 - 0}{0.09} = 8.7 $$

## Student's t-distribution 

The shaded area is the probability that we observe the value by chance (p-value)

```{r}
#| echo: false

# Set degrees of freedom
df <- nrow(data)-2

# Create a sequence of t-values
t_values <- seq(-5, 5, length.out = 300)

# Compute the density of these t-values
t_density <- dt(t_values, df)

# Create a data frame for plotting
plot_data <- data.frame(t_values, t_density)


# Plotting the t-distribution
ggplot(plot_data, aes(x = t_values, y = t_density)) +
  geom_line() +  # Draw the density line
  geom_area(data = plot_data %>% filter(t_values > 1.96), fill = "blue", alpha = 0.4) +  
  geom_area(data = plot_data %>% filter(t_values < -1.96), fill = "blue", alpha = 0.4) +  
  geom_vline(xintercept = 0,linetype="dashed") +
  labs(title = paste0("Density of t-distribution (df = ",df,")"),
       x = "t value",
       y = "Density") +
  theme_minimal(base_size = 15)  # Use a minimal theme

```


## Student's t-distribution 

Locating our t-statistic on the density of the t-distribution quantifies the probability that we observe our result by chance (p-val = 1.199041e-14)

```{r}
#| echo: false

# Set degrees of freedom
df <- nrow(data)-2

# Create a sequence of t-values
t_values <- seq(-5, 10, length.out = 300)

# Compute the density of these t-values
t_density <- dt(t_values, df)

# Create a data frame for plotting
plot_data <- data.frame(t_values, t_density)


# Plotting the t-distribution
ggplot(plot_data, aes(x = t_values, y = t_density)) +
  geom_line() +  # Draw the density line
  geom_point(aes(x=8.7,y=0)) +
  geom_area(data = plot_data %>% filter(t_values > 1.96), fill = "blue", alpha = 0.4) +  
  geom_area(data = plot_data %>% filter(t_values < -1.96), fill = "blue", alpha = 0.4) +  
  geom_vline(xintercept = 0,linetype="dashed") +
  labs(title = paste0("Density of t-distribution (df = ",df,")"),
       x = "t value",
       y = "Density") +
  theme_minimal(base_size = 15)  # Use a minimal theme


```

## 95% Confidence Interval

Reverse engineer the p-value of 0.05 to find the test statistic (1.96)


$$ 1.96 =  \frac{\beta - 0}{se}  $$

Then rearrange the t-statistics calculation

$$ upper = \beta + 1.96*se ; ~~~ lower = \beta - 1.96*se$$

## Inference - back to the trial

The p-val = 0.00000000000001199041 suggests the probability of observing the relationship between studying and test score by chance is small.

Formally, we reject the null hypothesis H0 (no relationship) at an alpha=.05

Our test does not prove that H1 is correct, but it provides strong evidence

## Interpreting regression coefficients

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false


m_out
```

:::

::: {.column width="50%"}

A student that does not study is expected to get a score of 40.389

A student's score should improve by 0.78 points per hour of additional studying

How many hours should a student study to get 80 points or better?

:::

::::

## Predicting new values

We can use the coefficient estimates to calculate expected scores from new students based on study time

$$scores=40 + 0.78 * hours $$

## Multivariate regression

You can add additional independent variables (aka regressors)

$$ y=\alpha + \beta_1 X_1 + \beta_2 X_2 + ... +\varepsilon $$

The $\beta$'s are **conditional** on the other covariates

Inference is similar


## Summary

Regression is a tool to estimate relationships between variables (measurements)

When assumptions are met, the regression provides the best estimates of the relationship

Hypothesis testing helps us understand the quality of our estimates






